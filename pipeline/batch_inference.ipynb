{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYfgJV2VQuUs",
        "outputId": "525b8eb8-60d0-428c-b823-88fcfedea54f"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics\n",
        "!pip install validators"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "from torchvision.models import efficientnet_v2_m as effnetv2m\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "vxX_R6ZqRaVy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======\n",
        "# device\n",
        "# ======\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "fwuqwUj3Qw6W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============\n",
        "# batch inference\n",
        "# ===============\n",
        "def batch_infer(\n",
        "    images, cls_custom=None, ci_custom=0.1, top_classes=5,\n",
        "    model_detector=\"yolov5x\", weight_detector=None,\n",
        "    model_classifier=\"effnetv2m\", weight_classifier=None\n",
        "):\n",
        "    # check image quantity\n",
        "    if len(images) != 2:\n",
        "        raise ValueError(\"only image quantity '2' supported for batch inference.\")\n",
        "\n",
        "    # check models\n",
        "    if model_detector != \"yolov5x\":\n",
        "        raise ValueError(\"only model 'yolov5x' supported for detector.\\n\")\n",
        "\n",
        "    if model_classifier != \"effnetv2m\":\n",
        "        raise ValueError(\"only model 'effnetv2m' is supported for classifier.\")\n",
        "\n",
        "    # load models\n",
        "    detector = torch.hub.load(\n",
        "        \"ultralytics/yolov5\",\n",
        "        \"custom\",\n",
        "        path=weight_detector,\n",
        "        force_reload=True\n",
        "    )\n",
        "    detector = detector.to(device)\n",
        "\n",
        "    classifier = effnetv2m()\n",
        "    # classifier.load_state_dict(torch.load(weight_classifier, map_location=device))\n",
        "\n",
        "    # detect and classify objects\n",
        "    outputs = []\n",
        "\n",
        "    for image in images:\n",
        "        # load image\n",
        "        with open(image, \"rb\") as f:\n",
        "            image = f.read()\n",
        "            arr = np.asarray(bytearray(image), dtype=np.uint8)\n",
        "            image = cv2.imdecode(arr, -1)\n",
        "\n",
        "        # check image\n",
        "        if image is None:\n",
        "            raise Exception(f\"no valid image read via 'cv2.imread()'.\\n\")\n",
        "\n",
        "        # detect objects\n",
        "        output = detector(image)\n",
        "        output = output.pandas().xyxy[0]\n",
        "        output = output[output[\"name\"] == cls_custom]\n",
        "\n",
        "        # check output\n",
        "        if output.empty:\n",
        "            raise Exception(f\"no '{cls_custom}' detected with confidence interval >= '{ci_custom}'.\")\n",
        "        else:\n",
        "            if output.shape[0] >= 2:\n",
        "                output = output.loc[[output[\"confidence\"].idxmax()]]\n",
        "\n",
        "        # calculate bounding box\n",
        "        xmin, xmax, ymin, ymax = output[\"xmin\"], output[\"xmax\"], output[\"ymin\"], output[\"ymax\"]\n",
        "\n",
        "        # crop image\n",
        "        image = image[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
        "\n",
        "        # configure transformation\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # transform image\n",
        "        image = transform(image)\n",
        "        image = image.unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = torch.nn.functional.softmax(classifier(image), dim=1)\n",
        "        utils = torch.hub.load(\n",
        "            \"NVIDIA/DeepLearningExamples:torchhub\",\n",
        "            \"nvidia_convnets_processing_utils\"\n",
        "        )\n",
        "        output = utils.pick_n_best(predictions=output, n=top_classes)\n",
        "        output = [cls[0] for cls in output[0]]\n",
        "\n",
        "        outputs.append(output)\n",
        "\n",
        "    output = [cls for cls in outputs[0] if cls in outputs[1]]\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "VkWUJUg-Qw8e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========\n",
        "# load files\n",
        "# ==========\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path_project = \"/content/drive/MyDrive/Animal-Detector\"\n",
        "\n",
        "path_weight = os.path.join(path_project, \"weights\")\n",
        "weight_detector = os.path.join(path_weight, \"yolov5x.pt\")\n",
        "weight_classifier = None\n",
        "\n",
        "image = os.path.join(path_project, \"image.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSkCxlSeQw-3",
        "outputId": "49db2bd7-3513-4561-9aaf-b51c96381643"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# test - batch inference\n",
        "# ======================\n",
        "output = batch_infer(\n",
        "    images=[image, image], cls_custom=\"Insecta\",\n",
        "    weight_detector=weight_detector, weight_classifier=weight_classifier\n",
        ")\n",
        "print(f\"output\\n{output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oRTyK-xQxBK",
        "outputId": "e1177fd3-bf63-4386-d41c-19a341355ca2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 ðŸš€ 2023-7-7 Python-3.10.12 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 322 layers, 86254162 parameters, 0 gradients\n",
            "Adding AutoShape... \n",
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample 0: [('pirate, pirate ship', '0.2%'), ('scuba diver', '0.2%'), ('coffee mug', '0.2%'), ('crate', '0.2%'), ('bottlecap', '0.1%')]\n",
            "sample 0: [('bottlecap', '0.2%'), ('scuba diver', '0.2%'), ('American chameleon, anole, Anolis carolinensis', '0.1%'), ('barrel, cask', '0.1%'), ('goblet', '0.1%')]\n",
            "output\n",
            "['scuba diver', 'bottlecap']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
          ]
        }
      ]
    }
  ]
}
